{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b821469-54d1-4d10-88c6-a719772b1e28",
   "metadata": {
    "id": "8b821469-54d1-4d10-88c6-a719772b1e28"
   },
   "source": [
    "Implemented a 2-layer neural network from scratch in NumPy to classify Iris flower species. Hand-coded forward and backward propagation using ReLU and softmax, and trained using cross-entropy loss. Achieved ~95% accuracy on the test set without using any deep learning libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baf3ef7-d749-45e0-9708-475c53e537a7",
   "metadata": {
    "id": "0baf3ef7-d749-45e0-9708-475c53e537a7"
   },
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4576f-3b8b-4802-9bce-2ca772e9a70b",
   "metadata": {
    "id": "94c4576f-3b8b-4802-9bce-2ca772e9a70b"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33ebbe05-d711-481b-93ce-43845217bba9",
   "metadata": {
    "id": "33ebbe05-d711-481b-93ce-43845217bba9"
   },
   "source": [
    "\\begin{array}{|c|l|}\n",
    "\\hline\n",
    "\\textbf{Symbol / Formula} & \\textbf{Meaning / Computation} \\\\\n",
    "\\hline\n",
    "w^{[1]} & \\text{Weights from input layer (layer 1) to hidden layer (layer 2)} \\\\\n",
    "(i, j) & \\text{Subscripts: } i = \\text{hidden neuron index},\\ j = \\text{input neuron index} \\\\\n",
    "w^{[1]}_{1,1} & \\text{Weight from } x_1 \\text{ to hidden neuron } a_1 \\\\\n",
    "w^{[1]}_{2,1} & \\text{Weight from } x_1 \\text{ to } a_2 \\\\\n",
    "w^{[2]}_{1,2} & \\text{Weight from hidden neuron } a_2 \\text{ to output neuron } y \\\\\n",
    "\\hline\n",
    "a_1 & w^{[1]}_{1,1}x_1 + w^{[1]}_{1,2}x_2 + b^{[1]}_1 \\\\\n",
    "    & = (0.5 \\cdot 2.3) + (0.1 \\cdot 10.2) + (-0.3) = 1.87 \\\\\n",
    "\\hline\n",
    "a_2 & w^{[1]}_{2,1}x_1 + w^{[1]}_{2,2}x_2 + b^{[1]}_2 \\\\\n",
    "    & = (-0.1 \\cdot 2.3) + (0.3 \\cdot 10.2) + 0.2 = 3.03 \\\\\n",
    "\\hline\n",
    "a_3 & w^{[1]}_{3,1}x_1 + w^{[1]}_{3,2}x_2 + b^{[1]}_3 \\\\\n",
    "    & = (0.2 \\cdot 2.3) + (-0.1 \\cdot 10.2) + 0.5 = 0.5 \\\\\n",
    "\\hline\n",
    "z^{[2]} &= w^{[2]}_{1,1} a_1 + w^{[2]}_{1,2} a_2 + w^{[2]}_{1,3} a_3 + b^{[2]}_1 \\\\\n",
    "        &= (-0.2 \\cdot 1.87) + (-0.3 \\cdot 3.03) + (-0.15 \\cdot 0.50) + 0.05 = -1.209 \\\\\n",
    "\\hline\n",
    "\\hat{y} &= \\sigma(z^{[2]}) = \\frac{1}{1 + e^{-z^{[2]}}} = \\frac{1}{1 + e^{-(-1.209)}} \\approx 0.23\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff8256-4ae8-4160-85fc-60a3512c726b",
   "metadata": {
    "id": "f0ff8256-4ae8-4160-85fc-60a3512c726b"
   },
   "source": [
    "### 🔹 Forward Pass\n",
    "| Step             | Formula                                 | Meaning                                 |\n",
    "|------------------|------------------------------------------|------------------------------------------|\n",
    "| Hidden Z         | $z_1 = X W_1 + b_1$                      | Pre-activation for hidden layer          |\n",
    "| Hidden A         | $a_1 = \\text{ReLU}(z_1)$                 | Activation output of hidden layer        |\n",
    "| Output Z         | $z_2 = a_1 W_2 + b_2$                    | Pre-activation for output layer          |\n",
    "| Output ŷ         | $\\hat{y} = \\sigma(z_2)$                  | Sigmoid output                           |\n",
    "### 🔹 Backward Pass\n",
    "| Step                  | Formula                                                | Meaning                                   |\n",
    "|------------------------|---------------------------------------------------------|--------------------------------------------|\n",
    "| Output delta           | $\\delta_2 = \\hat{y} - y$                                | Error at output                            |\n",
    "| Grad W₂                | $\\frac{\\partial L}{\\partial W_2} = a_1^T \\cdot \\delta_2$| Weight gradient from hidden to output      |\n",
    "| Grad b₂                | $\\frac{\\partial L}{\\partial b_2} = \\text{sum}(\\delta_2)$| Bias gradient for output                   |\n",
    "| Hidden delta           | $\\delta_1 = (\\delta_2 W_2^T) \\circ \\text{ReLU}'(z_1)$  | Error at hidden, ∘ = element-wise mult     |\n",
    "| Grad W₁                | $\\frac{\\partial L}{\\partial W_1} = X^T \\cdot \\delta_1$ | Weight gradient from input to hidden       |\n",
    "| Grad b₁                | $\\frac{\\partial L}{\\partial b_1} = \\text{sum}(\\delta_1)$| Bias gradient for hidden layer             |\n",
    "### 🔹 Weight Updates (Gradient Descent)\n",
    "| Parameter | Update Rule                                      |\n",
    "|-----------|--------------------------------------------------|\n",
    "| $W_1$     | $W_1 \\leftarrow W_1 - \\eta \\cdot \\frac{\\partial L}{\\partial W_1}$ |\n",
    "| $b_1$     | $b_1 \\leftarrow b_1 - \\eta \\cdot \\frac{\\partial L}{\\partial b_1}$ |\n",
    "| $W_2$     | $W_2 \\leftarrow W_2 - \\eta \\cdot \\frac{\\partial L}{\\partial W_2}$ |\n",
    "| $b_2$     | $b_2 \\leftarrow b_2 - \\eta \\cdot \\frac{\\partial L}{\\partial b_2}$ |\n",
    "### ✅ Mnemonic to Remember\n",
    "> \"**Z → A → Z → ŷ** (forward)  \n",
    ">  **ŷ − y → backprop through $W^T$ → apply activation' → gradients → update**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06509231-ff57-48e8-a261-01fc23b65a57",
   "metadata": {
    "id": "06509231-ff57-48e8-a261-01fc23b65a57"
   },
   "outputs": [],
   "source": [
    "# Forward\n",
    "z1 = W1 x + b1\n",
    "a1 = ReLU(z1)\n",
    "z2 = W2 a1 + b2\n",
    "ŷ  = sigmoid(z2)\n",
    "\n",
    "# Backward\n",
    "dL/dŷ = ŷ - y\n",
    "dL/dz2 = dL/dŷ * sigmoid'(z2)\n",
    "dL/dW2 = a1.T @ dL/dz2\n",
    "dL/db2 = dL/dz2\n",
    "\n",
    "dL/da1 = dL/dz2 @ W2.T\n",
    "dL/dz1 = dL/da1 * ReLU'(z1)\n",
    "dL/dW1 = x.T @ dL/dz1\n",
    "dL/db1 = dL/dz1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962cb0e-5b22-4226-9e8d-61637ca82af7",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb84a461-3a03-4195-87ed-c5dc1dde08b3",
   "metadata": {},
   "source": [
    "### Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b8fd807-1d54-4c5f-a22f-b0bec2aeb75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Activation functions and derivatives\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "# Loss function\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    eps = 1e-8  # avoid log(0)\n",
    "    return -np.mean(y_true * np.log(y_pred + eps) + (1 - y_true) * np.log(1 - y_pred + eps))\n",
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    eps = 1e-8\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred + eps), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbc9a08-e28a-4de7-8efc-3573b18a3679",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ba1c27-15df-42ad-84fc-37798ae2e6f7",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1749751510309,
     "user": {
      "displayName": "Jayshree Pillai",
      "userId": "07911816722962179958"
     },
     "user_tz": 240
    },
    "id": "24ba1c27-15df-42ad-84fc-37798ae2e6f7"
   },
   "outputs": [],
   "source": [
    "# Network class\n",
    "class FlexibleNN:\n",
    "    def __init__(self, input_dim, hidden_dim=3,output_dim=1):\n",
    "        # Initialize weights & biases\n",
    "        self.output_dim = output_dim\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2. / input_dim)\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.W2 = np.random.randn(hidden_dim, self.output_dim) * np.sqrt(2. / hidden_dim)\n",
    "        self.b2 = np.zeros((1, self.output_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x                           # Cache input\n",
    "        self.z1 = x @ self.W1 + self.b1      # Linear → Hidden layer\n",
    "        self.a1 = relu(self.z1)              # ReLU activation\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2 # Linear → Output\n",
    "        if self.output_dim == 1:\n",
    "            self.y_pred = sigmoid(self.z2)\n",
    "        else:\n",
    "            self.y_pred = softmax(self.z2)\n",
    "        return self.y_pred\n",
    "\n",
    "    def backward(self, y_true, learning_rate=0.01):\n",
    "        m = y_true.shape[0]\n",
    "        # Output layer\n",
    "        if self.output_dim == 1:     # ∂L/∂z2\n",
    "            dz2 = self.y_pred - y_true  # (N, 1)\n",
    "        else:\n",
    "            dz2 = self.y_pred - y_true  # (N, C)\n",
    "\n",
    "        dW2 = self.a1.T @ dz2 / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Hidden layer\n",
    "        da1 = dz2 @ self.W2.T                     # ∂L/∂a1\n",
    "        dz1 = da1 * relu_derivative(self.z1)      # ∂L/∂z1\n",
    "        dW1 = self.x.T @ dz1 / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Gradient descent\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "\n",
    "    def train(self, x, y_true, learning_rate=0.01):\n",
    "        y_pred = self.forward(x)\n",
    "        if self.output_dim == 1:\n",
    "            loss = binary_cross_entropy(y_true, y_pred)\n",
    "        else:\n",
    "            loss = categorical_cross_entropy(y_true, y_pred)\n",
    "\n",
    "        self.backward(y_true, learning_rate)\n",
    "        return y_pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab1b6bf-0c6a-4fbc-808b-b18a06e2ed94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1749751513774,
     "user": {
      "displayName": "Jayshree Pillai",
      "userId": "07911816722962179958"
     },
     "user_tz": 240
    },
    "id": "4ab1b6bf-0c6a-4fbc-808b-b18a06e2ed94",
    "outputId": "4bcbadd3-8692-45a1-dfd0-082f1c7ad9ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [[0.92181588]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[2.3, 10.2]])\n",
    "model = FlexibleNN(input_dim=2,output_dim=1)\n",
    "print(\"Output:\", model.forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bf9fb6-de6d-40e3-b4d7-792a3978ac90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1749751515394,
     "user": {
      "displayName": "Jayshree Pillai",
      "userId": "07911816722962179958"
     },
     "user_tz": 240
    },
    "id": "d4bf9fb6-de6d-40e3-b4d7-792a3978ac90",
    "outputId": "171efb6a-2d6d-4be1-df52-4022c63b3c90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output: 0.2473\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[2.3, 10.2]])  # shape (1, 2)\n",
    "model = FlexibleNN(input_dim=2, hidden_dim=3,output_dim=1)\n",
    "model.W1 = np.array([\n",
    "    [0.5, -0.1,0.2],  # weights from x1 to hidden neurons\n",
    "    [0.1, 0.3,-0.1],  # weights from x2 to hidden neurons\n",
    "])\n",
    "model.b1 = np.array([[-0.3, -0.2, 0.5]])\n",
    "model.W2 = np.array([\n",
    "    [-0.2],\n",
    "    [-0.3],\n",
    "    [-0.15]\n",
    "])\n",
    "model.b2 = np.array([[0.05]])\n",
    "y_hat = model.forward(x)\n",
    "print(f\"Predicted output: {y_hat[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b1ac2-d8c1-4da4-97f5-8967f264db32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11757,
     "status": "ok",
     "timestamp": 1749751528858,
     "user": {
      "displayName": "Jayshree Pillai",
      "userId": "07911816722962179958"
     },
     "user_tz": 240
    },
    "id": "6f8b1ac2-d8c1-4da4-97f5-8967f264db32",
    "outputId": "1bcd1428-0a07-4954-d169-221a5a76b456"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# 1. Load the data\n",
    "data = load_iris()\n",
    "X = data['data']            # shape (150, 4)\n",
    "y = data['target'].reshape(-1, 1)  # shape (150, 1)\n",
    "\n",
    "# 2. One-hot encode labels (for softmax output)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y)  # shape (150, 3)\n",
    "\n",
    "# 3. Train-test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Standardize features (fit on train only!)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = FlexibleNN(input_dim=4, hidden_dim=5, output_dim=3)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    _, loss = model.train(X_train, y_train, learning_rate=0.05)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "probs = model.forward(X_test)\n",
    "y_pred_labels = np.argmax(probs, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(y_pred_labels == y_true_labels)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5523c7-72d6-4ca3-89d4-d07a4a79d5fa",
   "metadata": {
    "executionInfo": {
     "elapsed": 9567,
     "status": "ok",
     "timestamp": 1749751540434,
     "user": {
      "displayName": "Jayshree Pillai",
      "userId": "07911816722962179958"
     },
     "user_tz": 240
    },
    "id": "1a5523c7-72d6-4ca3-89d4-d07a4a79d5fa"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, ReLU, Softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load and preprocess data\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target'].reshape(-1, 1)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# 2. Define model\n",
    "model = Sequential([\n",
    "    Dense(5, input_shape=(4,), activation='relu'),   # hidden layer\n",
    "    Dense(3, activation='softmax')                   # output layer\n",
    "])\n",
    "# 3. Compile\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.05),\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# 4. Train\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=32, verbose=0)\n",
    "\n",
    "# 5. Evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b3d51-c07d-4b74-8834-9dfea5afb1a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34967,
     "status": "ok",
     "timestamp": 1749751577948,
     "user": {
      "displayName": "Jayshree Pillai",
      "userId": "07911816722962179958"
     },
     "user_tz": 240
    },
    "id": "5d0b3d51-c07d-4b74-8834-9dfea5afb1a7",
    "outputId": "e1f5bae1-1582-4ead-eaf9-b52fcd22249f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d9955-9f3c-4adf-b926-6b0090c1695c",
   "metadata": {
    "id": "7b4d9955-9f3c-4adf-b926-6b0090c1695c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c535b8eb-c94e-476c-98d5-1c7516840dbb",
   "metadata": {
    "id": "c535b8eb-c94e-476c-98d5-1c7516840dbb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b3e58-84cf-49a3-b882-4c2031e14175",
   "metadata": {
    "id": "4d8b3e58-84cf-49a3-b882-4c2031e14175"
   },
   "outputs": [],
   "source": [
    "#C:\\Users\\vjs\\Desktop\\JobSearch\\ReviewNB\\ML\\JPProjects>jupyter nbconvert NeuralNetwork_Fundamentals.ipynb --to html --template classic"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
